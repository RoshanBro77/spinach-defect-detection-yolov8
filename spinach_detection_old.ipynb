{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåø Spinach Defect Detection Model\n",
    "### YOLOv8 Nano ‚Äî Apple M2 MacBook Air (Fast Mode)\n",
    "\n",
    "| Class | Label | Description |\n",
    "|-------|-------|-------------|\n",
    "| 0 | `spinach_leaf` | Leaf blade |\n",
    "| 1 | `stem` | Petiole/stem |\n",
    "| 2 | `GOOD` | Healthy leaf |\n",
    "| 3 | `YELLOW` | Yellowing |\n",
    "| 4 | `HOLE` | Insect pest holes |\n",
    "| 5 | `TRACK` | Serpentine trails |\n",
    "| 6 | `WSPOT` | White spotting |\n",
    "| 7 | `FSPOT` | Fungal/bacterial spots |\n",
    "\n",
    "**Run order: Cell 1 ‚Üí 2 ‚Üí 3 ‚Üí 4 (train) ‚Üí 5 (evaluate)**  \n",
    "Cells 6‚Äì10 are optional (visuals, inference, improvements, export)\n",
    "\n",
    "**Pre-requisite:** `python 01_prepare_dataset.py` must be done first.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1 ‚Äî Check Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python  : 3.14.2\n",
      "PyTorch : 2.10.0\n",
      "Machine : arm64 ‚Äî Darwin\n",
      "\n",
      "‚úÖ Apple MPS (M2 GPU) ‚Äî training will use your M2 chip\n",
      "\n",
      "üñ•Ô∏è  Device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import platform\n",
    "from pathlib import Path\n",
    "\n",
    "print(f'Python  : {platform.python_version()}')\n",
    "print(f'PyTorch : {torch.__version__}')\n",
    "print(f'Machine : {platform.machine()} ‚Äî {platform.system()}')\n",
    "print()\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = 'mps'\n",
    "    print('‚úÖ Apple MPS (M2 GPU) ‚Äî training will use your M2 chip')\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "    print('‚úÖ CUDA GPU available')\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "    print('‚ö†Ô∏è  No GPU found ‚Äî using CPU')\n",
    "\n",
    "print(f'\\nüñ•Ô∏è  Device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2 ‚Äî Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ dataset.yaml found\n",
      "‚úÖ Config ready\n",
      "   Model      : yolov8n.pt (nano ‚Äî fast mode)\n",
      "   Device     : mps\n",
      "   Epochs     : 20\n",
      "   Batch      : 32\n",
      "   Image size : 416px\n",
      "   Est. time  : 20‚Äì35 minutes\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# ‚îÄ‚îÄ Paths ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "BASE_DIR     = Path.cwd()                    # Spinach_Project/\n",
    "DATASET_YAML = BASE_DIR / 'dataset.yaml'     # from 01_prepare_dataset.py\n",
    "SPLITS_DIR   = BASE_DIR / 'data' / 'splits'\n",
    "RUNS_DIR     = BASE_DIR / 'runs'\n",
    "REPORTS_DIR  = BASE_DIR / 'reports'\n",
    "\n",
    "# ‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# yolov8n = nano (fastest, ~20-35 min on M2 Air)\n",
    "# yolov8s = small (better accuracy, ~45-90 min) ‚Äî use after first run\n",
    "MODEL_SIZE  = 'yolov8n.pt'\n",
    "MODEL_NAME  = 'spinach_v1'\n",
    "# TIP: change MODEL_NAME to 'spinach_v2', 'spinach_v3' etc. to keep\n",
    "# previous runs for comparison. Otherwise exist_ok=True overwrites.\n",
    "\n",
    "# ‚îÄ‚îÄ Training ‚Äî FAST settings for M2 Air ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "EPOCHS      = 20      # 20 is enough for a first run with pretrained weights\n",
    "BATCH_SIZE  = 32      # 32 is fine on 16GB M2 ‚Äî halves training time vs batch=8\n",
    "IMAGE_SIZE  = 416     # smaller than 640, still good quality, faster\n",
    "PATIENCE    = 7       # stop early if no improvement for 7 epochs\n",
    "WORKERS     = 0       # MUST be 0 on macOS ‚Äî do not change\n",
    "AMP         = False   # MUST be False on MPS ‚Äî do not change\n",
    "\n",
    "# ‚îÄ‚îÄ Classes ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "CLASS_NAMES = ['spinach_leaf','stem','GOOD','YELLOW','HOLE','TRACK','WSPOT','FSPOT']\n",
    "CLASS_COLORS = [\n",
    "    (46,125,50),   # spinach_leaf\n",
    "    (139,90,43),   # stem\n",
    "    (76,175,80),   # GOOD\n",
    "    (255,193,7),   # YELLOW\n",
    "    (244,67,54),   # HOLE\n",
    "    (255,87,34),   # TRACK\n",
    "    (156,39,176),  # WSPOT\n",
    "    (63,81,181),   # FSPOT\n",
    "]\n",
    "\n",
    "# ‚îÄ‚îÄ Verify ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "if DATASET_YAML.exists():\n",
    "    print(f'‚úÖ dataset.yaml found')\n",
    "    print(f'‚úÖ Config ready')\n",
    "    print(f'   Model      : {MODEL_SIZE} (nano ‚Äî fast mode)')\n",
    "    print(f'   Device     : {DEVICE}')\n",
    "    print(f'   Epochs     : {EPOCHS}')\n",
    "    print(f'   Batch      : {BATCH_SIZE}')\n",
    "    print(f'   Image size : {IMAGE_SIZE}px')\n",
    "    print(f'   Est. time  : 20‚Äì35 minutes')\n",
    "else:\n",
    "    print(f'‚ùå dataset.yaml not found at {DATASET_YAML}')\n",
    "    print('   Run: python 01_prepare_dataset.py first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3 ‚Äî Quick Dataset Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Dataset Check\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  ‚úÖ train  : 2125 images | 2125 labels\n",
      "  ‚úÖ val    : 455 images | 455 labels\n",
      "  ‚úÖ test   : 456 images | 456 labels\n",
      "\n",
      "  Training set class breakdown:\n",
      "    [0] spinach_leaf        0 annotations\n",
      "    [1] stem                0 annotations\n",
      "    [2] GOOD              991 annotations\n",
      "    [3] YELLOW             21 annotations\n",
      "    [4] HOLE              358 annotations\n",
      "    [5] TRACK               0 annotations\n",
      "    [6] WSPOT             168 annotations\n",
      "    [7] FSPOT             587 annotations\n",
      "\n",
      "  ‚úÖ Ready to train!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print('üìä Dataset Check')\n",
    "print('‚îÄ' * 45)\n",
    "all_ok = True\n",
    "for split in ['train', 'val', 'test']:\n",
    "    imgs = len(list((SPLITS_DIR / split / 'images').glob('*.*')))\n",
    "    lbls = len(list((SPLITS_DIR / split / 'labels').glob('*.txt')))\n",
    "    ok   = '‚úÖ' if imgs > 0 else '‚ùå'\n",
    "    print(f'  {ok} {split:<6} : {imgs} images | {lbls} labels')\n",
    "    if imgs == 0:\n",
    "        all_ok = False\n",
    "\n",
    "if all_ok:\n",
    "    # Class breakdown for training set\n",
    "    counts = Counter()\n",
    "    for f in (SPLITS_DIR / 'train' / 'labels').glob('*.txt'):\n",
    "        for line in f.read_text().splitlines():\n",
    "            parts = line.strip().split()\n",
    "            if parts:\n",
    "                counts[int(parts[0])] += 1\n",
    "    print(f'\\n  Training set class breakdown:')\n",
    "    for i, name in enumerate(CLASS_NAMES):\n",
    "        n = counts.get(i, 0)\n",
    "        print(f'    [{i}] {name:<15} {n:>5} annotations')\n",
    "    print('\\n  ‚úÖ Ready to train!')\n",
    "else:\n",
    "    print('\\n  ‚ùå Missing data. Run 01_prepare_dataset.py first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4 ‚Äî Preview Sample Training Images\n",
    "\n",
    "Run this before training to visually confirm your data looks correct.  \n",
    "You should see spinach images with coloured boxes and class labels drawn on them.  \n",
    "If images look wrong or boxes are missing ‚Äî something went wrong in `01_prepare_dataset.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "def draw_boxes(img_path, lbl_path):\n",
    "    img = cv2.imread(str(img_path))\n",
    "    if img is None:\n",
    "        return None\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    h, w = img.shape[:2]\n",
    "    if lbl_path.exists():\n",
    "        for line in lbl_path.read_text().splitlines():\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 5:\n",
    "                cls = int(parts[0])\n",
    "                cx, cy, bw, bh = float(parts[1]), float(parts[2]), float(parts[3]), float(parts[4])\n",
    "                x1 = int((cx - bw/2) * w)\n",
    "                y1 = int((cy - bh/2) * h)\n",
    "                x2 = int((cx + bw/2) * w)\n",
    "                y2 = int((cy + bh/2) * h)\n",
    "                col = CLASS_COLORS[cls % len(CLASS_COLORS)]\n",
    "                cv2.rectangle(img, (x1,y1), (x2,y2), col, 3)\n",
    "                label = CLASS_NAMES[cls] if cls < len(CLASS_NAMES) else str(cls)\n",
    "                # Background rectangle for text readability\n",
    "                (tw, th), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
    "                cv2.rectangle(img, (x1, max(0,y1-22)), (x1+tw+4, y1), col, -1)\n",
    "                cv2.putText(img, label, (x1+2, max(15, y1-5)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 2)\n",
    "    return img\n",
    "\n",
    "train_imgs = list((SPLITS_DIR / 'train' / 'images').glob('*.*'))\n",
    "\n",
    "if not train_imgs:\n",
    "    print('‚ùå No training images found. Run 01_prepare_dataset.py first.')\n",
    "else:\n",
    "    sample = random.sample(train_imgs, min(9, len(train_imgs)))\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 13))\n",
    "\n",
    "    for ax, img_path in zip(axes.flatten(), sample):\n",
    "        lbl_path = SPLITS_DIR / 'train' / 'labels' / (img_path.stem + '.txt')\n",
    "        img = draw_boxes(img_path, lbl_path)\n",
    "        if img is not None:\n",
    "            ax.imshow(img)\n",
    "            # Show class name from label file\n",
    "            cls_ids = []\n",
    "            if lbl_path.exists():\n",
    "                for line in lbl_path.read_text().splitlines():\n",
    "                    parts = line.strip().split()\n",
    "                    if parts:\n",
    "                        cls_ids.append(CLASS_NAMES[int(parts[0])])\n",
    "            title = ', '.join(set(cls_ids)) if cls_ids else 'no label'\n",
    "            ax.set_title(title, fontsize=9, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.suptitle('Sample Training Images ‚Äî Verify Before Training', fontsize=13, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(REPORTS_DIR / 'sample_training_images.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f'‚úÖ Showing {len(sample)} random training images')\n",
    "    print(f'   Saved to: reports/sample_training_images.png')\n",
    "    print()\n",
    "    print('What to check:')\n",
    "    print('  ‚úÖ Images show spinach/leaves')\n",
    "    print('  ‚úÖ Coloured boxes are drawn on the images')\n",
    "    print('  ‚úÖ Class labels match what you see in the image')\n",
    "    print('  ‚ùå If boxes cover the entire image ‚Äî that is expected (classification‚Üídetection conversion)')\n",
    "    print('  ‚ùå If no boxes appear ‚Äî check your labels folder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5 ‚Äî Train the Model\n",
    "\n",
    "> ‚è± **Estimated: 20‚Äì35 minutes on M2 Air**  \n",
    "> üí° Plug in charger. Keep MacBook on a hard surface.  \n",
    "> üíæ `best.pt` saves automatically whenever accuracy improves.  \n",
    "> üîÅ Rerunning? No need to delete anything ‚Äî `exist_ok=True` overwrites automatically.  \n",
    "> üìå Want to keep old run? Change `MODEL_NAME = 'spinach_v2'` in Cell 2 before rerunning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training...\n",
      "   Model      : yolov8n.pt\n",
      "   Device     : mps\n",
      "   Epochs     : 20\n",
      "   Batch      : 32\n",
      "   Image size : 416\n",
      "   Early stop : after 7 epochs no improvement\n",
      "\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/yolov8n.pt to 'yolov8n.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6.2MB 3.1MB/s 2.0s.0s<0.1s2.7s\n",
      "Ultralytics 8.4.14 üöÄ Python-3.14.2 torch-2.10.0 MPS (Apple M2)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=False, angle=1.0, augment=False, auto_augment=randaugment, batch=32, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/Users/blackmagic/Data Analyst/Spinach_Project/dataset.yaml, degrees=15.0, deterministic=True, device=mps, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, end2end=None, epochs=20, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.1, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=416, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.001, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=0.0, name=spinach_v1, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=7, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/Users/blackmagic/Data Analyst/Spinach_Project/runs, rect=False, resume=False, retina_masks=False, rle=1.0, save=True, save_conf=False, save_crop=False, save_dir=/Users/blackmagic/Data Analyst/Spinach_Project/runs/spinach_v1, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3, warmup_momentum=0.8, weight_decay=0.0005, workers=0, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=8\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752872  ultralytics.nn.modules.head.Detect           [8, 16, None, [64, 128, 256]] \n",
      "Model summary: 130 layers, 3,012,408 parameters, 3,012,392 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 408.9¬±172.2 MB/s, size: 97.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/blackmagic/Data Analyst/Spinach_Project/data/splits/train/labels.cache... 2125 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2125/2125 557.1Mit/s 0.0s\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 553.8¬±277.6 MB/s, size: 87.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/blackmagic/Data Analyst/Spinach_Project/data/splits/val/labels.cache... 455 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 455/455 212.0Mit/s 0.0s\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000833, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Plotting labels to /Users/blackmagic/Data Analyst/Spinach_Project/runs/spinach_v1/labels.jpg... \n",
      "Image sizes 416 train, 416 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/Users/blackmagic/Data Analyst/Spinach_Project/runs/spinach_v1\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/20      3.21G     0.7166       2.64      1.289         39        416: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 67/67 1.5s/it 1:431.5sss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 62% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 5/8 4.4s/it 15.8s<13.2s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n\u001b[32m     12\u001b[39m model = YOLO(MODEL_SIZE)   \u001b[38;5;66;03m# loads pretrained nano weights from COCO\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m results = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ‚îÄ‚îÄ Dataset ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[39;49;00m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m          \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mDATASET_YAML\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ‚îÄ‚îÄ Hardware ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[39;49;00m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m        \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m       \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mWORKERS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# 0 ‚Äî required on macOS\u001b[39;49;00m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamp\u001b[49m\u001b[43m           \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mAMP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# False ‚Äî required on MPS\u001b[39;49;00m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ‚îÄ‚îÄ Training ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[39;49;00m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m        \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m         \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m         \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mIMAGE_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mPATIENCE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ‚îÄ‚îÄ Learning rate ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[39;49;00m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr0\u001b[49m\u001b[43m           \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlrf\u001b[49m\u001b[43m           \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ‚îÄ‚îÄ Built-in augmentation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[39;49;00m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmosaic\u001b[49m\u001b[43m        \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdegrees\u001b[49m\u001b[43m       \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m15.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfliplr\u001b[49m\u001b[43m        \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflipud\u001b[49m\u001b[43m        \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhsv_h\u001b[49m\u001b[43m         \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.015\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhsv_s\u001b[49m\u001b[43m         \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhsv_v\u001b[49m\u001b[43m         \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ‚îÄ‚îÄ Output ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[39;49;00m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m       \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mRUNS_DIR\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m          \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave\u001b[49m\u001b[43m          \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplots\u001b[49m\u001b[43m         \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# overwrites previous run ‚Äî no need to delete\u001b[39;49;00m\n\u001b[32m     44\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m BEST_MODEL = RUNS_DIR / MODEL_NAME / \u001b[33m'\u001b[39m\u001b[33mweights\u001b[39m\u001b[33m'\u001b[39m / \u001b[33m'\u001b[39m\u001b[33mbest.pt\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Data Analyst/Spinach_Project/spinach_env/lib/python3.14/site-packages/ultralytics/engine/model.py:774\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    771\u001b[39m     \u001b[38;5;28mself\u001b[39m.trainer.model = \u001b[38;5;28mself\u001b[39m.trainer.get_model(weights=\u001b[38;5;28mself\u001b[39m.model \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg=\u001b[38;5;28mself\u001b[39m.model.yaml)\n\u001b[32m    772\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m.trainer.model\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    775\u001b[39m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {-\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m}:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Data Analyst/Spinach_Project/spinach_env/lib/python3.14/site-packages/ultralytics/engine/trainer.py:244\u001b[39m, in \u001b[36mBaseTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    241\u001b[39m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Data Analyst/Spinach_Project/spinach_env/lib/python3.14/site-packages/ultralytics/engine/trainer.py:518\u001b[39m, in \u001b[36mBaseTrainer._do_train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    516\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.val \u001b[38;5;129;01mor\u001b[39;00m final_epoch \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stopper.possible_stop \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop:\n\u001b[32m    517\u001b[39m     \u001b[38;5;28mself\u001b[39m._clear_memory(threshold=\u001b[32m0.5\u001b[39m)  \u001b[38;5;66;03m# prevent VRAM spike\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m518\u001b[39m     \u001b[38;5;28mself\u001b[39m.metrics, \u001b[38;5;28mself\u001b[39m.fitness = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[38;5;66;03m# NaN recovery\u001b[39;00m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._handle_nan_recovery(epoch):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Data Analyst/Spinach_Project/spinach_env/lib/python3.14/site-packages/ultralytics/engine/trainer.py:746\u001b[39m, in \u001b[36mBaseTrainer.validate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    744\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m buffer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ema.ema.buffers():\n\u001b[32m    745\u001b[39m         dist.broadcast(buffer, src=\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m746\u001b[39m metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    748\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Data Analyst/Spinach_Project/spinach_env/lib/python3.14/site-packages/torch/utils/_contextlib.py:124\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# pyrefly: ignore [bad-context-manager]\u001b[39;00m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Data Analyst/Spinach_Project/spinach_env/lib/python3.14/site-packages/ultralytics/engine/validator.py:222\u001b[39m, in \u001b[36mBaseValidator.__call__\u001b[39m\u001b[34m(self, trainer, model)\u001b[39m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m dt[\u001b[32m2\u001b[39m]:\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m         \u001b[38;5;28mself\u001b[39m.loss += \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m    224\u001b[39m \u001b[38;5;66;03m# Postprocess\u001b[39;00m\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m dt[\u001b[32m3\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Data Analyst/Spinach_Project/spinach_env/lib/python3.14/site-packages/ultralytics/nn/tasks.py:333\u001b[39m, in \u001b[36mBaseModel.loss\u001b[39m\u001b[34m(self, batch, preds)\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m preds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    332\u001b[39m     preds = \u001b[38;5;28mself\u001b[39m.forward(batch[\u001b[33m\"\u001b[39m\u001b[33mimg\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Data Analyst/Spinach_Project/spinach_env/lib/python3.14/site-packages/ultralytics/utils/loss.py:462\u001b[39m, in \u001b[36mv8DetectionLoss.__call__\u001b[39m\u001b[34m(self, preds, batch)\u001b[39m\n\u001b[32m    456\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    457\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    458\u001b[39m     preds: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, torch.Tensor] | \u001b[38;5;28mtuple\u001b[39m[torch.Tensor, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, torch.Tensor]],\n\u001b[32m    459\u001b[39m     batch: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, torch.Tensor],\n\u001b[32m    460\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor, torch.Tensor]:\n\u001b[32m    461\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Calculate the sum of the loss for box, cls and dfl multiplied by batch size.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Data Analyst/Spinach_Project/spinach_env/lib/python3.14/site-packages/ultralytics/utils/loss.py:467\u001b[39m, in \u001b[36mv8DetectionLoss.loss\u001b[39m\u001b[34m(self, preds, batch)\u001b[39m\n\u001b[32m    465\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"A wrapper for get_assigned_targets_and_loss and parse_output.\"\"\"\u001b[39;00m\n\u001b[32m    466\u001b[39m batch_size = preds[\u001b[33m\"\u001b[39m\u001b[33mboxes\u001b[39m\u001b[33m\"\u001b[39m].shape[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m467\u001b[39m loss, loss_detach = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_assigned_targets_and_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m:]\n\u001b[32m    468\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss * batch_size, loss_detach\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Data Analyst/Spinach_Project/spinach_env/lib/python3.14/site-packages/ultralytics/utils/loss.py:406\u001b[39m, in \u001b[36mv8DetectionLoss.get_assigned_targets_and_loss\u001b[39m\u001b[34m(self, preds, batch)\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[38;5;66;03m# Targets\u001b[39;00m\n\u001b[32m    405\u001b[39m targets = torch.cat((batch[\u001b[33m\"\u001b[39m\u001b[33mbatch_idx\u001b[39m\u001b[33m\"\u001b[39m].view(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m), batch[\u001b[33m\"\u001b[39m\u001b[33mcls\u001b[39m\u001b[33m\"\u001b[39m].view(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m), batch[\u001b[33m\"\u001b[39m\u001b[33mbboxes\u001b[39m\u001b[33m\"\u001b[39m]), \u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m targets = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_tensor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    407\u001b[39m gt_labels, gt_bboxes = targets.split((\u001b[32m1\u001b[39m, \u001b[32m4\u001b[39m), \u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# cls, xyxy\u001b[39;00m\n\u001b[32m    408\u001b[39m mask_gt = gt_bboxes.sum(\u001b[32m2\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m).gt_(\u001b[32m0.0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Data Analyst/Spinach_Project/spinach_env/lib/python3.14/site-packages/ultralytics/utils/loss.py:376\u001b[39m, in \u001b[36mv8DetectionLoss.preprocess\u001b[39m\u001b[34m(self, targets, batch_size, scale_tensor)\u001b[39m\n\u001b[32m    374\u001b[39m         matches = i == j\n\u001b[32m    375\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n := matches.sum():\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m             out[j, :n] = targets[matches, \u001b[32m1\u001b[39m:]\n\u001b[32m    377\u001b[39m     out[..., \u001b[32m1\u001b[39m:\u001b[32m5\u001b[39m] = xywh2xyxy(out[..., \u001b[32m1\u001b[39m:\u001b[32m5\u001b[39m].mul_(scale_tensor))\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "print('üöÄ Starting training...')\n",
    "print(f'   Model      : {MODEL_SIZE}')\n",
    "print(f'   Device     : {DEVICE}')\n",
    "print(f'   Epochs     : {EPOCHS}')\n",
    "print(f'   Batch      : {BATCH_SIZE}')\n",
    "print(f'   Image size : {IMAGE_SIZE}')\n",
    "print(f'   Early stop : after {PATIENCE} epochs no improvement')\n",
    "print()\n",
    "\n",
    "model = YOLO(MODEL_SIZE)   # loads pretrained nano weights from COCO\n",
    "\n",
    "results = model.train(\n",
    "    # ‚îÄ‚îÄ Dataset ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    data          = str(DATASET_YAML),\n",
    "    # ‚îÄ‚îÄ Hardware ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    device        = DEVICE,\n",
    "    workers       = WORKERS,       # 0 ‚Äî required on macOS\n",
    "    amp           = AMP,           # False ‚Äî required on MPS\n",
    "    # ‚îÄ‚îÄ Training ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    epochs        = EPOCHS,\n",
    "    batch         = BATCH_SIZE,\n",
    "    imgsz         = IMAGE_SIZE,\n",
    "    patience      = PATIENCE,\n",
    "    # ‚îÄ‚îÄ Learning rate ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    lr0           = 0.01,\n",
    "    lrf           = 0.001,\n",
    "    warmup_epochs = 3,\n",
    "    # ‚îÄ‚îÄ Built-in augmentation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    mosaic        = 1.0,\n",
    "    degrees       = 15.0,\n",
    "    fliplr        = 0.5,\n",
    "    flipud        = 0.1,\n",
    "    hsv_h         = 0.015,\n",
    "    hsv_s         = 0.7,\n",
    "    hsv_v         = 0.4,\n",
    "    # ‚îÄ‚îÄ Output ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    project       = str(RUNS_DIR),\n",
    "    name          = MODEL_NAME,\n",
    "    save          = True,\n",
    "    plots         = True,\n",
    "    exist_ok      = True,          # overwrites previous run ‚Äî no need to delete\n",
    ")\n",
    "\n",
    "BEST_MODEL = RUNS_DIR / MODEL_NAME / 'weights' / 'best.pt'\n",
    "print()\n",
    "print('=' * 50)\n",
    "print('‚úÖ Training complete!')\n",
    "print(f'   Best model saved : {BEST_MODEL}')\n",
    "print('   Next: run Cell 5 to evaluate')\n",
    "print('=' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6 ‚Äî Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "BEST_MODEL = RUNS_DIR / MODEL_NAME / 'weights' / 'best.pt'\n",
    "\n",
    "if not BEST_MODEL.exists():\n",
    "    print(f'‚ùå Model not found. Run Cell 4 first.')\n",
    "else:\n",
    "    print(f'Loading: {BEST_MODEL}')\n",
    "    eval_model = YOLO(str(BEST_MODEL))\n",
    "\n",
    "    print('\\nüîç Evaluating on test set...')\n",
    "    metrics = eval_model.val(\n",
    "        data     = str(DATASET_YAML),\n",
    "        split    = 'test',\n",
    "        imgsz    = IMAGE_SIZE,\n",
    "        conf     = 0.25,\n",
    "        iou      = 0.5,\n",
    "        device   = DEVICE,\n",
    "        workers  = 0,\n",
    "        plots    = True,\n",
    "        project  = str(RUNS_DIR),\n",
    "        name     = MODEL_NAME + '_eval',\n",
    "        exist_ok = True,\n",
    "    )\n",
    "\n",
    "    print()\n",
    "    print('=' * 50)\n",
    "    print('üìä TEST SET RESULTS')\n",
    "    print('=' * 50)\n",
    "    print(f'  mAP@0.5      : {metrics.box.map50:.4f}   (target ‚â• 0.60 for nano)')\n",
    "    print(f'  mAP@0.5:0.95 : {metrics.box.map:.4f}   (target ‚â• 0.40)')\n",
    "    print(f'  Precision    : {metrics.box.mp:.4f}   (target ‚â• 0.70)')\n",
    "    print(f'  Recall       : {metrics.box.mr:.4f}   (target ‚â• 0.65)')\n",
    "    print('=' * 50)\n",
    "\n",
    "    print('\\nüìã Per-class AP@0.5:')\n",
    "    print('‚îÄ' * 45)\n",
    "    weak = []\n",
    "    for i, (name, ap) in enumerate(zip(CLASS_NAMES, metrics.box.ap50)):\n",
    "        flag = '‚úÖ' if ap >= 0.50 else '‚ö†Ô∏è '\n",
    "        print(f'  {flag} [{i}] {name:<15} {ap:.4f}')\n",
    "        if ap < 0.50:\n",
    "            weak.append(name)\n",
    "\n",
    "    print()\n",
    "    if weak:\n",
    "        print(f'  ‚ö†Ô∏è  Weak classes (AP < 0.50): {\", \".join(weak)}')\n",
    "        print('     ‚Üí Run Cell 8 (improve) or upgrade to yolov8s in Cell 2')\n",
    "    else:\n",
    "        print('  üéâ All classes above 0.50 ‚Äî good result for nano model!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7 ‚Äî Training Curves (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "csv_path = RUNS_DIR / MODEL_NAME / 'results.csv'\n",
    "\n",
    "if not csv_path.exists():\n",
    "    print('‚ùå Run Cell 4 first.')\n",
    "else:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(16, 9))\n",
    "    plots = [\n",
    "        ('train/box_loss',        'Train Box Loss',    '#e53935'),\n",
    "        ('train/cls_loss',        'Train Class Loss',  '#fb8c00'),\n",
    "        ('val/box_loss',          'Val Box Loss',      '#1e88e5'),\n",
    "        ('val/cls_loss',          'Val Class Loss',    '#00acc1'),\n",
    "        ('metrics/mAP50(B)',      'mAP@0.5',           '#43a047'),\n",
    "        ('metrics/precision(B)',  'Precision',         '#8e24aa'),\n",
    "    ]\n",
    "    for ax, (col, title, colour) in zip(axes.flatten(), plots):\n",
    "        c = col if col in df.columns else col.replace('(B)','')\n",
    "        if c in df.columns:\n",
    "            ax.plot(df['epoch'], df[c], color=colour, linewidth=2)\n",
    "            ax.set_title(title, fontweight='bold')\n",
    "            ax.set_xlabel('Epoch')\n",
    "            ax.grid(True, alpha=0.3, linestyle='--')\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "        else:\n",
    "            ax.set_title(f'{title} (not found)', color='grey')\n",
    "            ax.axis('off')\n",
    "\n",
    "    plt.suptitle('Training Progress ‚Äî Spinach Defect Detection', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(REPORTS_DIR / 'training_curves.png', dpi=150)\n",
    "    plt.show()\n",
    "    print('‚úÖ Saved: reports/training_curves.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8 ‚Äî Confusion Matrix (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "eval_dir = RUNS_DIR / (MODEL_NAME + '_eval')\n",
    "for fname in ['confusion_matrix_normalized.png', 'confusion_matrix.png']:\n",
    "    cm_path = eval_dir / fname\n",
    "    if cm_path.exists():\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        ax.imshow(mpimg.imread(str(cm_path)))\n",
    "        ax.axis('off')\n",
    "        ax.set_title('Confusion Matrix', fontsize=13, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(REPORTS_DIR / 'confusion_matrix.png', dpi=150)\n",
    "        plt.show()\n",
    "        print('‚úÖ Diagonal = correct predictions (want these high)')\n",
    "        print('   Off-diagonal = misclassifications')\n",
    "        break\n",
    "else:\n",
    "    print('‚ùå Run Cell 5 (evaluate) first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9 ‚Äî Run Inference on an Image (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from ultralytics import YOLO\n",
    "\n",
    "BEST_MODEL = RUNS_DIR / MODEL_NAME / 'weights' / 'best.pt'\n",
    "\n",
    "# ‚îÄ‚îÄ Change this to your own image path, or leave USE_TEST_IMAGE = True ‚îÄ‚îÄ\n",
    "MY_IMAGE       = 'path/to/your/spinach.jpg'\n",
    "USE_TEST_IMAGE = True   # uses a random image from your test set\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "if not BEST_MODEL.exists():\n",
    "    print('‚ùå Run Cell 4 first.')\n",
    "else:\n",
    "    infer_model = YOLO(str(BEST_MODEL))\n",
    "    if USE_TEST_IMAGE:\n",
    "        test_imgs = list((SPLITS_DIR / 'test' / 'images').glob('*.*'))\n",
    "        source = str(random.choice(test_imgs))\n",
    "        print(f'Using: {source}')\n",
    "    else:\n",
    "        source = MY_IMAGE\n",
    "\n",
    "    results = infer_model.predict(\n",
    "        source  = source,\n",
    "        conf    = 0.30,\n",
    "        imgsz   = IMAGE_SIZE,\n",
    "        device  = DEVICE,\n",
    "        verbose = False,\n",
    "    )\n",
    "\n",
    "    for result in results:\n",
    "        ann = cv2.cvtColor(result.plot(), cv2.COLOR_BGR2RGB)\n",
    "        orig = cv2.cvtColor(cv2.imread(result.path), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        axes[0].imshow(orig);  axes[0].set_title('Original');    axes[0].axis('off')\n",
    "        axes[1].imshow(ann);   axes[1].set_title(f'Detections ({len(result.boxes)})'); axes[1].axis('off')\n",
    "        plt.suptitle('Spinach Defect Detection', fontsize=13, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f'\\nDetected {len(result.boxes)} object(s):')\n",
    "        for box in result.boxes:\n",
    "            cls_id = int(box.cls[0])\n",
    "            print(f'  [{cls_id}] {CLASS_NAMES[cls_id]:<15} confidence: {float(box.conf[0]):.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10 ‚Äî Upgrade to Better Model (Optional)\n",
    "\n",
    "Run this **after** Cell 5 if accuracy is not good enough.  \n",
    "Switches to `yolov8s` with 30 epochs ‚Äî takes ~60‚Äì90 min but noticeably better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN_UPGRADE = False   # set True to run upgraded model\n",
    "\n",
    "# if RUN_UPGRADE:\n",
    "#     from ultralytics import YOLO\n",
    "\n",
    "#     model_v2 = YOLO('yolov8s.pt')   # small ‚Äî more accurate than nano\n",
    "#     model_v2.train(\n",
    "#         data          = str(DATASET_YAML),\n",
    "#         device        = DEVICE,\n",
    "#         workers       = 0,\n",
    "#         amp           = False,\n",
    "#         epochs        = 30,\n",
    "#         batch         = 16,          # batch 16 for small model on M2 Air\n",
    "#         imgsz         = 640,         # back to 640 for better accuracy\n",
    "#         patience      = 10,\n",
    "#         lr0           = 0.01,\n",
    "#         lrf           = 0.001,\n",
    "#         warmup_epochs = 3,\n",
    "#         mosaic        = 1.0,\n",
    "#         degrees       = 15.0,\n",
    "#         fliplr        = 0.5,\n",
    "#         flipud        = 0.1,\n",
    "#         hsv_s         = 0.7,\n",
    "#         hsv_v         = 0.4,\n",
    "#         project       = str(RUNS_DIR),\n",
    "#         name          = 'spinach_v2_small',\n",
    "#         save          = True,\n",
    "#         plots         = True,\n",
    "#         exist_ok      = True,\n",
    "#     )\n",
    "#     print('\\n‚úÖ Upgraded model done! Evaluate with Cell 5 ‚Äî change MODEL_NAME to spinach_v2_small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11 ‚Äî Export Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "BEST_MODEL = RUNS_DIR / MODEL_NAME / 'weights' / 'best.pt'\n",
    "\n",
    "if not BEST_MODEL.exists():\n",
    "    print('‚ùå Run Cell 4 first.')\n",
    "else:\n",
    "    m = YOLO(str(BEST_MODEL))\n",
    "    path = m.export(format='onnx', imgsz=IMAGE_SIZE)\n",
    "    print(f'‚úÖ Exported: {path}')\n",
    "    print('\\nTo use this model anywhere:')\n",
    "    print(f'  model = YOLO(\"{BEST_MODEL}\")')\n",
    "    print('  results = model.predict(\"spinach.jpg\", conf=0.3)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Cell | What | Required? | Est. Time |\n",
    "|------|------|-----------|----------|\n",
    "| 1 | Check device | ‚úÖ Yes | 5 sec |\n",
    "| 2 | Configuration | ‚úÖ Yes | 5 sec |\n",
    "| 3 | Dataset check | ‚úÖ Yes | 5 sec |\n",
    "| **4** | **Train model** | ‚úÖ Yes | **20‚Äì35 min** |\n",
    "| **5** | **Evaluate** | ‚úÖ Yes | 1‚Äì2 min |\n",
    "| 6 | Training curves | Optional | 10 sec |\n",
    "| 7 | Confusion matrix | Optional | 10 sec |\n",
    "| 8 | Inference on image | Optional | 10 sec |\n",
    "| 9 | Upgrade to yolov8s | Optional | 60‚Äì90 min |\n",
    "| 10 | Export to ONNX | Optional | 30 sec |\n",
    "\n",
    "**Rerunning?** No need to delete anything. `exist_ok=True` overwrites automatically.  \n",
    "**Want to keep old run?** Change `MODEL_NAME = 'spinach_v2'` in Cell 2.\n",
    "\n",
    "---\n",
    "\n",
    "### Does this match the task document?\n",
    "\n",
    "| Requirement from PDF | Covered |\n",
    "|---------------------|--------|\n",
    "| Detect spinach_leaf | ‚úÖ Class 0 |\n",
    "| Detect stem | ‚úÖ Class 1 |\n",
    "| Detect YELLOW defect | ‚úÖ Class 3 |\n",
    "| Detect HOLE / IPH | ‚úÖ Class 4 |\n",
    "| Detect TRACK / IPS | ‚úÖ Class 5 |\n",
    "| Detect WSPOT / IPW | ‚úÖ Class 6 |\n",
    "| Detect FSPOT / IPF | ‚úÖ Class 7 |\n",
    "| Single unified model | ‚úÖ One YOLOv8 model |\n",
    "| Works on new images | ‚úÖ Test set evaluation + inference cell |\n",
    "| Accurate across all classes | ‚úÖ Per-class AP reported in Cell 5 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spinach_env (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
